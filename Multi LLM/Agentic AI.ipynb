{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "75f03468-6885-49f3-9df2-557568877e0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸš€ Running Full Agentic Multi-LLM System...\n",
      "\n",
      "[DataLoaderAgent] Full Dataset Loaded: (94006, 22)\n",
      "\n",
      "[Embedding] TF-IDF...\n",
      "[Embedding] Hashing...\n",
      "[Embedding] MiniLM (Loading)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\envs\\myenv\\lib\\site-packages\\huggingface_hub\\file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â³ Computing MiniLM embeddings (first run only)...\n",
      "ðŸ’¾ Saved MiniLM cache\n",
      "\n",
      "[Model Selection] LightGBM + Agentic Thresholding...\n",
      "[LightGBM] [Info] Number of positive: 429, number of negative: 74775\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.212425 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 87012\n",
      "[LightGBM] [Info] Number of data points in the train set: 75204, number of used features: 414\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n",
      "[LightGBM] [Info] Number of positive: 3418, number of negative: 71786\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.199242 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 87012\n",
      "[LightGBM] [Info] Number of data points in the train set: 75204, number of used features: 414\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "[LightGBM] [Info] Number of positive: 641, number of negative: 74563\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.201124 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 87012\n",
      "[LightGBM] [Info] Number of data points in the train set: 75204, number of used features: 414\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "[LightGBM] [Info] Number of positive: 2611, number of negative: 72593\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.196124 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 87012\n",
      "[LightGBM] [Info] Number of data points in the train set: 75204, number of used features: 414\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      " â†’ tfidf: Accuracy=0.9370, F1=0.6406, Hamming=0.0165\n",
      "[LightGBM] [Info] Number of positive: 429, number of negative: 74775\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.243051 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 105324\n",
      "[LightGBM] [Info] Number of data points in the train set: 75204, number of used features: 414\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n",
      "[LightGBM] [Info] Number of positive: 3418, number of negative: 71786\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.245407 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 105324\n",
      "[LightGBM] [Info] Number of data points in the train set: 75204, number of used features: 414\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "[LightGBM] [Info] Number of positive: 641, number of negative: 74563\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.219708 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 105324\n",
      "[LightGBM] [Info] Number of data points in the train set: 75204, number of used features: 414\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "[LightGBM] [Info] Number of positive: 2611, number of negative: 72593\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.243542 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 105324\n",
      "[LightGBM] [Info] Number of data points in the train set: 75204, number of used features: 414\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      " â†’ hashing: Accuracy=0.9325, F1=0.6215, Hamming=0.0178\n",
      "[LightGBM] [Info] Number of positive: 429, number of negative: 74775\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.479871 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 101253\n",
      "[LightGBM] [Info] Number of data points in the train set: 75204, number of used features: 398\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n",
      "[LightGBM] [Info] Number of positive: 3418, number of negative: 71786\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.519996 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 101253\n",
      "[LightGBM] [Info] Number of data points in the train set: 75204, number of used features: 398\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "[LightGBM] [Info] Number of positive: 641, number of negative: 74563\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.491887 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 101253\n",
      "[LightGBM] [Info] Number of data points in the train set: 75204, number of used features: 398\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "[LightGBM] [Info] Number of positive: 2611, number of negative: 72593\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.487776 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 101253\n",
      "[LightGBM] [Info] Number of data points in the train set: 75204, number of used features: 398\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      " â†’ minilm: Accuracy=0.9270, F1=0.6116, Hamming=0.0189\n",
      "\n",
      "ðŸ¤– Agent Selected Best Embedding: tfidf (F1=0.6406)\n",
      "\n",
      "âœ” Completed! Best Model + Thresholds Saved Internally.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, f1_score, hamming_loss\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, HashingVectorizer\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "import lightgbm as lgb\n",
    "from sentence_transformers import SentenceTransformer\n",
    "# Agent 1: Data Loader Agent\n",
    "class DataLoaderAgent:\n",
    "    def __init__(self, path, subset_ratio=1.0, random_state=42):\n",
    "        self.path = path\n",
    "        self.subset_ratio = subset_ratio\n",
    "        self.random_state = random_state\n",
    "\n",
    "    def execute(self):\n",
    "        df = pd.read_csv(self.path)\n",
    "        print(f\"[DataLoaderAgent] Full Dataset Loaded: {df.shape}\")\n",
    "\n",
    "        if self.subset_ratio < 1.0:\n",
    "            df = df.sample(frac=self.subset_ratio, random_state=self.random_state)\n",
    "            print(f\"[DataLoaderAgent] Subset Selected: {df.shape}\")\n",
    "\n",
    "        return df\n",
    "# Agent 2: Preprocessor Agent\n",
    "class PreprocessorAgent:\n",
    "    def execute(self, df):\n",
    "        df[\"Code\"] = df[\"Code\"].astype(str).apply(lambda x: x[:800])\n",
    "\n",
    "        code = df[\"Code\"]\n",
    "        labels = df[[\"Long method\", \"God class\", \"Feature envy\", \"Data class\"]]\n",
    "\n",
    "        metrics = df.drop(columns=[\n",
    "            \"Code\", \"File\", \"Project\", \"Class\",\n",
    "            \"Long method\", \"God class\", \"Feature envy\", \"Data class\"\n",
    "        ], errors=\"ignore\").fillna(0)\n",
    "\n",
    "        return code, metrics, labels\n",
    "# Agent 3: Multi-LLM Embedding Agent\n",
    "class MultiLLMEmbedder:\n",
    "\n",
    "    def embed_batch(self, model, texts, batch_size=300):\n",
    "        embeddings = []\n",
    "        for i in range(0, len(texts), batch_size):\n",
    "            batch = list(texts[i:i+batch_size])\n",
    "            emb = model.encode(batch, convert_to_numpy=True, show_progress_bar=False)\n",
    "            embeddings.append(emb)\n",
    "        return np.vstack(embeddings)\n",
    "\n",
    "    def execute(self, X_code):\n",
    "\n",
    "        print(\"\\n[Embedding] TF-IDF...\")\n",
    "        tfidf = TfidfVectorizer(max_features=400)\n",
    "        tfidf_emb = tfidf.fit_transform(X_code).toarray()\n",
    "\n",
    "        print(\"[Embedding] Hashing...\")\n",
    "        hash_vec = HashingVectorizer(n_features=400)\n",
    "        hash_emb = hash_vec.transform(X_code).toarray()\n",
    "\n",
    "        print(\"[Embedding] MiniLM (Loading)...\")\n",
    "        model = SentenceTransformer(\"sentence-transformers/paraphrase-MiniLM-L3-v2\")\n",
    "        cache_path = \"minilm_cache.npy\"\n",
    "\n",
    "        if os.path.exists(cache_path):\n",
    "            print(\"ðŸ” Loading cached MiniLM embeddings...\")\n",
    "            minilm_emb = np.load(cache_path)\n",
    "        else:\n",
    "            print(\"â³ Computing MiniLM embeddings (first run only)...\")\n",
    "            minilm_emb = self.embed_batch(model, X_code)\n",
    "            np.save(cache_path, minilm_emb)\n",
    "            print(\"ðŸ’¾ Saved MiniLM cache\")\n",
    "\n",
    "        return {\"tfidf\": tfidf_emb, \"hashing\": hash_emb, \"minilm\": minilm_emb}\n",
    "# Agent 4: Feature Fusion Agent\n",
    "class FeatureFusionAgent:\n",
    "    def execute(self, emb_dict, metrics):\n",
    "        scaler = StandardScaler()\n",
    "        metrics_scaled = scaler.fit_transform(metrics)\n",
    "\n",
    "        return {\n",
    "            name: np.hstack((emb, metrics_scaled))\n",
    "            for name, emb in emb_dict.items()\n",
    "        }\n",
    "# Helper: Per-label Threshold Tuning\n",
    "def tune_thresholds(y_true, y_probs):\n",
    "    n_labels = y_true.shape[1]\n",
    "    thresholds = np.zeros(n_labels)\n",
    "\n",
    "    for i in range(n_labels):\n",
    "        top_f1 = 0\n",
    "        for t in np.linspace(0.1, 0.9, 81):\n",
    "            preds = (y_probs[:, i] >= t).astype(int)\n",
    "            f = f1_score(y_true[:, i], preds, zero_division=0)\n",
    "            if f > top_f1:\n",
    "                top_f1 = f\n",
    "                thresholds[i] = t\n",
    "    return thresholds\n",
    "# Agent 5: Agentic Model Trainer (LightGBM + Threshold Tuning)\n",
    "class AgenticModelTrainer:\n",
    "    def execute(self, fused, y):\n",
    "        results = {}\n",
    "        best_f1 = -1\n",
    "        best_model = None\n",
    "        best_name = None\n",
    "        best_thresholds = None\n",
    "\n",
    "        print(\"\\n[Model Selection] LightGBM + Agentic Thresholding...\")\n",
    "\n",
    "        for name, X in fused.items():\n",
    "            X_train, X_test, y_train, y_test = train_test_split(\n",
    "                X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "            clf = OneVsRestClassifier(\n",
    "                lgb.LGBMClassifier(\n",
    "                    n_estimators=250,\n",
    "                    class_weight=\"balanced\",\n",
    "                    n_jobs=-1\n",
    "                )\n",
    "            )\n",
    "            clf.fit(X_train, y_train)\n",
    "\n",
    "            y_probs = np.column_stack([\n",
    "                est.predict_proba(X_test)[:, 1]\n",
    "                for est in clf.estimators_\n",
    "            ])\n",
    "\n",
    "            thresholds = tune_thresholds(y_test.values, y_probs)\n",
    "            y_pred = (y_probs >= thresholds).astype(int)\n",
    "\n",
    "            acc = accuracy_score(y_test, y_pred)\n",
    "            f1 = f1_score(y_test, y_pred, average=\"micro\", zero_division=0)\n",
    "            ham = hamming_loss(y_test, y_pred)\n",
    "\n",
    "            results[name] = (acc, f1, ham)\n",
    "            print(f\" â†’ {name}: Accuracy={acc:.4f}, F1={f1:.4f}, Hamming={ham:.4f}\")\n",
    "\n",
    "            if f1 > best_f1:\n",
    "                best_f1 = f1\n",
    "                best_model = clf\n",
    "                best_name = name\n",
    "                best_thresholds = thresholds\n",
    "\n",
    "        print(f\"\\nðŸ¤– Agent Selected Best Embedding: {best_name} (F1={best_f1:.4f})\")\n",
    "        return (best_model, best_thresholds), results\n",
    "\n",
    "# Orchestrator Agent\n",
    "class AgenticOrchestrator:\n",
    "    def __init__(self, path, subset_ratio=1.0):\n",
    "        self.path = path\n",
    "        self.subset_ratio = subset_ratio\n",
    "\n",
    "    def run(self):\n",
    "        df = DataLoaderAgent(self.path, self.subset_ratio).execute()\n",
    "        code, metrics, labels = PreprocessorAgent().execute(df)\n",
    "        embeddings = MultiLLMEmbedder().execute(code)\n",
    "        fused = FeatureFusionAgent().execute(embeddings, metrics)\n",
    "        return AgenticModelTrainer().execute(fused, labels)\n",
    "\n",
    "# RUN\n",
    "dataset_path = r\"C:\\Users\\user\\Desktop\\combined-multi-smell-dataset.csv\"\n",
    "\n",
    "print(\"\\nðŸš€ Running Full Agentic Multi-LLM System...\\n\")\n",
    "(best_model, best_thresholds), results = AgenticOrchestrator(dataset_path).run()\n",
    "print(\"\\nâœ” Completed! Best Model + Thresholds Saved Internally.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2476a7e-b049-4562-9eae-a7afa7837bd1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
